{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee2073a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/rushikesh/python_files/vscode/transformer/src/main/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a59f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.set_printoptions(\n",
    "    precision=2,     # number of decimals\n",
    "    linewidth=120,   # max characters per line\n",
    "    sci_mode=False   # disable scientific notation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "from utils.Embeddings.Embeddings import Embeddings\n",
    "from utils.Attention.Attention import attentionLayer\n",
    "from utils.Attention.Attention import multiHeads\n",
    "from utils.Attention.Attention import residualConnection\n",
    "from utils.FFN.FFN import feedForward\n",
    "from utils.Tokenizer.Tokenizer import SimpleTokenizer\n",
    "from utils.LayerNorm.LayerNorm import LayerNorm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee3643",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e421f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Text.txt\", 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_words(word_list):\n",
    "    cleaned = []\n",
    "    for word in word_list:\n",
    "        word_lower = word.lower()  # convert to lowercase\n",
    "        if re.fullmatch(r\"[a-z]+\", word_lower) and (len(word_lower) >=4) and (len(word_lower) <= 8) :\n",
    "            cleaned.append(word_lower)\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01508408",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = clean_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some words from the training data\")\n",
    "print(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = words.copy()\n",
    "\n",
    "new_words = []\n",
    "seven_letter_count = 0\n",
    "data = []\n",
    "\n",
    "for w in words:\n",
    "    if len(w) == 7:\n",
    "        seven_letter_count += 1\n",
    "        if seven_letter_count < 10000:\n",
    "            new_words.append(w)   \n",
    "        else:\n",
    "            data.append(w) # keep from 10000th onward\n",
    "    else:\n",
    "        new_words.append(w)       # keep non-7-letter words\n",
    "\n",
    "# Replace words\n",
    "words = new_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def prepare_training_example(word, tokenizer, max_len=8, mask_prob=0.5):\n",
    "   \n",
    "    padded_word = list(word) + ['<e>' for i in range(max_len - len(word))]\n",
    "\n",
    "    x_chars = padded_word[::]\n",
    "\n",
    "    for i in range(len(x_chars)):\n",
    "        if x_chars[i] != '<e>' and (x_chars[i] != '<s>') and random.random() < mask_prob:\n",
    "            x_chars[i] = '<mask>'\n",
    "    \n",
    "    x_encoded = tokenizer.encode(x_chars, pad=True)\n",
    "\n",
    "    candidates = [i for i in range(len(x_chars)) if ((x_chars[i] not in ['<e>', '<s>']) & (x_chars[i] == '<mask>'))]\n",
    "\n",
    "    y_chars = list(x_chars)\n",
    "\n",
    "    if candidates:\n",
    "\n",
    "        idx = random.choice(candidates)\n",
    "        \n",
    "        y_chars[idx] = padded_word[idx]\n",
    "\n",
    "    y_encoded = tokenizer.encode(list(y_chars), pad=True)\n",
    "    \n",
    "    return x_encoded, y_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4d1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence   = 'mango'\n",
    "\n",
    "Tokenizer  = SimpleTokenizer(list('abcdefghijklmnopqrstuvwxyz'))\n",
    "\n",
    "x          = Tokenizer.encode(sequence, pad = True)\n",
    "\n",
    "\n",
    "print(\"Example usage of the simple encoder\")\n",
    "\n",
    "print(f\"Original word: {sequence}\")\n",
    "\n",
    "print(f\"Tokenized word: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e250106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "X, Y = [], []\n",
    "\n",
    "for word in words:\n",
    "    # Choose a random mask probability for this example\n",
    "    mask_prob = random.uniform(0.05, 0.3)  # e.g., 10% to 80% masked\n",
    "    u = random.random()\n",
    "    # mask_prob = 0.05 + (0.8 - 0.1) * (1 - u**2)  # favor smaller mask fraction\n",
    "    \n",
    "    x, y = prepare_training_example(word, Tokenizer, max_len=8, mask_prob=mask_prob)\n",
    "    \n",
    "    X.append(x)\n",
    "\n",
    "    Y.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 333\n",
    "print(f\"Example word: {words[idx]}\\n\")\n",
    "\n",
    "print(f\"Input sequence:  {X[idx].squeeze()}\")\n",
    "print(f\"Decoded Input sequence:  {\"\".join(Tokenizer.itos[i] for i in X[idx].squeeze().numpy())}\\n\")\n",
    "\n",
    "\n",
    "print(f\"Target sequence:  {Y[idx].squeeze()}\")\n",
    "print(f\"Decoded Target sequence:  {\"\".join(Tokenizer.itos[i] for i in Y[idx].squeeze().numpy())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc99ecd",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f912243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- prepared training examples ---\n",
    "# X: list of tensors (B, T)\n",
    "# Y: list of tensors (B, T)\n",
    "# Convert to tensors\n",
    "X_tensor = torch.stack(X)  # shape: (B, T)\n",
    "Y_tensor = torch.stack(Y)  # shape: (B, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "seq_len    = 10 ## max word lenght = 8 + 2 tokens (start and end)\n",
    "vocab_size = 29 ## 26 alphabets + (start, end and mask)\n",
    "d_model    = 128\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.005\n",
    "\n",
    "\n",
    "emb        = Embeddings(vocab_size = vocab_size, d_model= d_model, max_len= seq_len)\n",
    "\n",
    "multiHeadLayer = multiHeads(num_heads = 4, \n",
    "                            d_model   = d_model, \n",
    "                            max_len   = seq_len)\n",
    "rc = residualConnection()\n",
    "FFN = feedForward(d_model = d_model, max_len = seq_len, vocab_size = vocab_size)\n",
    "L1  = LayerNorm(d_model, seq_len)\n",
    "\n",
    "# --- Loss and optimizer ---\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(emb.parameters() +\n",
    "                       multiHeadLayer.parameters() +\n",
    "                       FFN.parameters() + L1.parameters(),\n",
    "                       lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ff3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Training loop ---\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "display(fig)\n",
    "\n",
    "# loss_history = []\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    idx  = random.sample(range(len(words)), 2500)\n",
    "\n",
    "    X_idx = X_tensor[idx].squeeze(1)\n",
    "    Y_idx = Y_tensor[idx].squeeze(1)\n",
    "\n",
    "    \n",
    "    # Forward pass\n",
    "    x0 = emb(X_idx)\n",
    "    x1 = multiHeadLayer(x0)\n",
    "    x2 = rc(x0, x1)\n",
    "    x3 = L1(x2)\n",
    "    x4 = FFN(x3)\n",
    "    logits = x4\n",
    "    targets = Y_idx\n",
    "    ## Loss on all tokens -------- OPTION A\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # Compute loss\n",
    "    # loss = loss_fn(\n",
    "    #     logits.view(-1, logits.size(-1)),  # (B*T, V)\n",
    "    #     targets.view(-1)                    # (B*T)\n",
    "    # )\n",
    "\n",
    "    ## Loss on just revealed mask ------ OPTION B\n",
    "\n",
    "    mask = (X_idx == 28) & (Y_idx <= 26)\n",
    "\n",
    "    mask_flat = mask.view(-1)\n",
    "\n",
    "    masked_logits = logits.view(-1, logits.size(-1))[mask_flat]\n",
    "    masked_targets = targets.view(-1)[mask_flat]\n",
    "\n",
    "    loss = loss_fn(masked_logits, masked_targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        if loss.item() < 4:\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            ax.clear()\n",
    "            ax.plot(loss_history)\n",
    "            ax.set_title(\"Training Loss\")\n",
    "            ax.set_xlabel(\"Epoch\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.grid(True)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0106838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loss_history), 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d2c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(emb, multiHeadLayer, rc, L1, FFN, x_single):\n",
    "    \"\"\"\n",
    "    X_single: shape (seq_len,) integer tensor with possibly some 28-mask tokens.\n",
    "    \"\"\"\n",
    "    x = x_single.unsqueeze(0)  # (1, seq_len)\n",
    "    \n",
    "    # Forward pass (same as training)\n",
    "    x0 = emb(x)\n",
    "    x1 = multiHeadLayer(x0)\n",
    "    x2 = rc(x0, x1)\n",
    "    x3 = L1(x2)\n",
    "    logits = FFN(x3)   # (1, seq_len, vocab)\n",
    "\n",
    "    # Find mask positions\n",
    "    mask = (x == 28).squeeze(0)   # (seq_len,)\n",
    "\n",
    "    preds = []\n",
    "    for i in range(len(mask)):\n",
    "        if mask[i]:\n",
    "            token_logits = logits[0, i]      # (vocab_size,)\n",
    "            pred_id = token_logits.argmax().item()\n",
    "            preds.append(pred_id)\n",
    "        else:\n",
    "            preds.append(x_single[i].item())  # copy original token\n",
    "\n",
    "    return torch.tensor(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3039\n",
    "\n",
    "print(\"Example word:\", words[idx])\n",
    "print(\"Input sequence: \", X[idx].squeeze())\n",
    "print(\"Decoded input:  \", \"\".join(Tokenizer.itos[i] for i in X[idx].squeeze().numpy()))\n",
    "print()\n",
    "\n",
    "pred_ids = predict(emb, multiHeadLayer, rc, L1, FFN, X_tensor[idx].squeeze())\n",
    "print(\"Predicted seq: \", pred_ids)\n",
    "print(\"Decoded prediction:\", \"\".join(Tokenizer.itos[i] for i in pred_ids.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd1ff4",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0\n",
    "data = []\n",
    "for w in words:\n",
    "    if len(w) == 7:\n",
    "        if ct > 10000:\n",
    "            data.append(w)\n",
    "        ct += 1\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_sequence(secret_word, correct):\n",
    "    \"\"\"\n",
    "    correct: list of correctly guessed characters\n",
    "    \"\"\"\n",
    "    seq = []\n",
    "    for ch in secret_word:\n",
    "        if ch in correct:\n",
    "            seq.append(Tokenizer.stoi[ch])\n",
    "        else:\n",
    "            seq.append(28)  # mask\n",
    "    return torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_letter_from_model(secret_word, missed, correct,\n",
    "                          emb, multiHeadLayer, rc, L1, FFN):\n",
    "    x_single = build_input_sequence(secret_word, correct)\n",
    "    x = x_single.unsqueeze(0)  # (1, seq_len)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x0 = emb(x)\n",
    "        x1 = multiHeadLayer(x0)\n",
    "        x2 = rc(x0, x1)\n",
    "        x3 = L1(x2)\n",
    "        logits = FFN(x3)  # (1, seq_len, vocab)\n",
    "\n",
    "    mask_positions = (x_single == 28)\n",
    "\n",
    "    # Sum logits over masked positions\n",
    "    letter_logits = torch.zeros(logits.size(-1))\n",
    "    for i in torch.where(mask_positions)[0]:\n",
    "        letter_logits += logits[0, i]\n",
    "\n",
    "    probs = F.softmax(letter_logits, dim=0)\n",
    "\n",
    "    # Avoid guessing already tried letters\n",
    "    tried = set(missed + correct)\n",
    "    for ch in tried:\n",
    "        probs[Tokenizer.stoi[ch]] = 0.0\n",
    "\n",
    "    probs = probs / probs.sum()  # renormalize\n",
    "\n",
    "    guess_id = torch.argmax(probs).item()\n",
    "    return Tokenizer.itos[guess_id]\n",
    "\n",
    "\n",
    "def simulate_hangman_games_model(data,\n",
    "                                 emb, multiHeadLayer, rc, L1, FFN,\n",
    "                                 max_misses=6):\n",
    "    wins = 0\n",
    "    total = len(data)\n",
    "\n",
    "    for secret_word in data[10000:]:\n",
    "        missed, correct = [], []\n",
    "        game_over = False\n",
    "\n",
    "        while not game_over:\n",
    "            guess = get_letter_from_model(\n",
    "                secret_word, missed, correct,\n",
    "                emb, multiHeadLayer, rc, L1, FFN\n",
    "            )\n",
    "\n",
    "            if guess in secret_word:\n",
    "                correct.append(guess)\n",
    "                if all(c in correct for c in secret_word):\n",
    "                    wins += 1\n",
    "                    game_over = True\n",
    "            else:\n",
    "                missed.append(guess)\n",
    "                if len(missed) >= max_misses:\n",
    "                    game_over = True\n",
    "\n",
    "    return wins, total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48425fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wins, total = simulate_hangman_games_model( data, emb, multiHeadLayer, rc, L1, FFN)\n",
    "\n",
    "print(f\"Wins: {wins}, Total: {total}\")\n",
    "print(f\"Win rate: {wins / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552f387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
